---
title: QEUR22_FLUX06:　Flux閑話休題～ディープラーニングをわかりやすく解説する
date: 2022-11-10
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング"]
excerpt: JuliaとFluxを使った機械学習
---

## QEUR22_FLUX06:　Flux閑話休題～ディープラーニングをわかりやすく解説する

## ～　これは分かりやすい。おススメです。　～

QEU:FOUNDER （設定65歳） ： “2012年から、ひたすら空気を読まず「我が道を行く」・・・。さて、この番組は**「高齢者によるイノベーション」**です。**さぁ！！脱「後進国」を目指して、進めぇー！！！**”

[![MOVIE1](http://img.youtube.com/vi/SIIzDVYnjFY/0.jpg)](http://www.youtube.com/watch?v=SIIzDVYnjFY "○ The News ● 円安、物価高、低賃金… 先進国から転落する日本 〜個人や企業、若者はどうすべきか【野口悠紀雄・望月衣塑子・尾形聡彦】")

D先生 （設定65歳）： “また、世の中の空気を読んでいない発言を・・・。多くのオッサンは、まだ「こんなこと（↓）」を考えていますよ。”

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER  ： “まあ、しようがないね。**そんな人たちだから・・・（笑）**。さて、今回は閑話休題でコレをやります。”

![imageJL1-66-1](/2022-11-10-QEUR22_FLUX06/imageJL1-66-1.jpg)

D先生 ： “え～っつ！？これをやるんですか？”

QEU:FOUNDER ： “これとよく似た内容のブログは数か所で見ました。ブログの出来が良いので、あちこちで紹介されているんだろうね。我々は**遣唐使**をやりましょう。あっ、ちなみに小生は3日に1回はアウトプットを出す（ブログを書く）ことにしているんで・・・。。”

D先生 ： “それは律儀な心掛けですね。でも・・・、なぜですか？”

QEU:FOUNDER ： “これ（↓）からの引用だったかなぁ・・・。1980年代に巨大な成功を収めたアップルが1990年代で倒産寸前までに追い詰められたのは、開発指向のあまり「製品リリースをしなかった」から・・・。やりたいことがあって、それがすべて達成されるまでモノを世に出さなかったんです。このようにすると、実はプロジェクトが迷走しやすいんです・・・。”

![imageJL1-66-2](/2022-11-10-QEUR22_FLUX06/imageJL1-66-2.jpg)

D先生 ： “（プロジェクトは）少しでも細かく区切って、リリースしたほうが良いですよね。多少は遅くなっても・・・。”

QEU:FOUNDER ： “あと、「ソフトウェアをオープンにすること」が健全さの維持にとって、とても大切です・・・。あのころのアップルはOOP（オブジェクト指向プログラミング）の開発で躓き、システム開発が一向に進まなかったそうだ。その後iMacで復興したのは、それらのプロジェクトを放棄し、動画再生(Quick TIme)技術をリリースしてからでした・・・。これからのアップルはぶれていないですよね。それが彼らの「強み」なのかな・・・。さてさて、例によってプログラムをドン！結果もドン！さらに、途中で解説もはさみます。”

```julia
# FLUXでディープラーニングをデモする
using Flux
using Flux: Descent, train!, Dense
# この関数がディープラーニングのwx+bに相当します。
f(x) = x + 1

x_train = [0 1 2 3 4]
y_train = f.(x_train)

x_test = [5 6 7 8 9]
y_test = f.(x_test)

```

![imageJL1-66-3](/2022-11-10-QEUR22_FLUX06/imageJL1-66-3.jpg)

```julia
# 构建预测模型
# この関数がディープラーニングのwx+bに相当します。
# 这是自动初始化的结果
model = Dense(1, 1) #构建训练模型
model.weight #权重
model.bias   #偏量

```

![imageJL1-66-4](/2022-11-10-QEUR22_FLUX06/imageJL1-66-4.jpg)

QEU:FOUNDER ： “この「Dense命令」の定義が大切です。この定義で、y=ax+bという単純な一次関数の近似になりました。ここで、aは重みであり、bはバイアスです。初期値設定では、ランダムに定義した重みとバイアスが上記のようになりました。”

D先生 ： “このように、関数の係数を繰り返しによって回帰精度を上げるのは**「勾配降下(GD:Gradient Descent)法」**になりますよね。”

QEU:FOUNDER ： “だから、あとでoptインスタンスにDescentメソッドが呼び出されていますよね。”

```julia
# 初期の予測準備
predict = model

# 这是因为Dense(1, 1)
# 实现了功能σ(Wx+b)
predict(x_train)

```

![imageJL1-66-5](/2022-11-10-QEUR22_FLUX06/imageJL1-66-5.jpg)

```julia
# 设置损失函数，使用均方差mse
loss(x,y)= Flux.Losses.mse(predict(x),y)
# 可以看一下现在的loss
loss(x_train, y_train)

```

QEU:FOUNDER ： “損失関数の定義も大切です。”

![imageJL1-66-6](/2022-11-10-QEUR22_FLUX06/imageJL1-66-6.jpg)

D先生 ： “Jeremy Howardのモデル図より明らかです。”

```julia
# 接下来设置训练数据与训练参数
parameters = Flux.params(predict)

```

![imageJL1-66-7](/2022-11-10-QEUR22_FLUX06/imageJL1-66-7.jpg)

```julia
# いままでが、最初のトライアルです。
# これからは、パラメタを更新させます
# Stochatic Gradient Descent
# 通过train!来训练，训练后查看loss
#using Flux: Descent, train! Dense
opt = Descent()
data = [(x_train, y_train)]
train!(loss, parameters, data, opt)
loss(x_train, y_train)

```

QEU:FOUNDER ： “以前、勾配降下法をやってみました。若干、複雑なプログラムだったと思います。今回のプログラムがこれほど簡単になったのは、Fluxパッケージの**「train!命令」**の力です。”

D先生 ： “具体的には、当てはめた関数の傾き（微分）を計算し、パラメタ（重み、バイアス）のアップデートをしているということですね。”

```julia
# 好像还不错？再看看测试集的结果怎么样
predict(x_test)

```

![imageJL1-66-8](/2022-11-10-QEUR22_FLUX06/imageJL1-66-8.jpg)

```julia
# 改进预测, 训练个10次
for i in 1:10
    train!(loss, parameters, data, opt)
    println(loss(x_train, y_train))
end

```

![imageJL1-66-9](/2022-11-10-QEUR22_FLUX06/imageJL1-66-9.jpg)

```julia
# 最后看一下参数
# 関数パラメタがx+1であるべき
parameters

# 看看测试集的结果怎么样
predict(x_test)

```

![imageJL1-66-10](/2022-11-10-QEUR22_FLUX06/imageJL1-66-10.jpg)

D先生 ： “あとは、繰り返しによって当てはめ精度が改善されるというわけですね。これは、Fluxを使った勾配降下法の簡単な説明です。いやあ、わかりやすかった・・・。”

### （ディープラーニングの構成）
- **ノード（多層）構造**
- **活性化関数**
- **勾配降下法**

QEU:FOUNDER ： “あとは、このデモプログラムにノード構造と活性化関数を組み入れれば、ディープラーニングができます。Fluxって簡単ですね・・・。”

D先生 ： “MLJFluxって、わざわざやる必要あるのですかねェ・・・。”

QEU:FOUNDER ： “すべての機械学習を同じ手順にまとめるというアラン・チューリング研究所の野心、Ｋｅｒａｓのようにハイバーパラメタを最適化する便利さはあるよね。”

D先生 ： “次回は、回帰問題をMLJ Fluxを使ってやるんですよね。できますか・・・？”

QEU:FOUNDER ： “さあ・・・（笑）。”


## ～　まとめ　～

QEU:FOUNDER ： “ああ・・・。この話には長い歴史があるんだねえ・・・。”

![imageJL1-66-11](/2022-11-10-QEUR22_FLUX06/imageJL1-66-11.jpg)

C部長 : “大河ドラマに三国志が重なったように・・・（笑）。”

QEU:FOUNDER ： “もともとは、A国の問題なんだよねえ・・・。J国が勝手に問題を何十倍も大きくしちゃったような感じです。”

![imageJL1-66-12](/2022-11-10-QEUR22_FLUX06/imageJL1-66-12.jpg)

C部長 : “「〇〇ゲート」って、結局何だったんでしょうね・・・。”

QEU:FOUNDER ： “小生も良くわからない。意外と、この辺のうまくまとまった記事が手に入らなかった。1970年代後半なので、朝鮮戦争の終結処理とベトナム戦争の関与が大きなモチベーションになっているという感じです。”

![imageJL1-66-13](/2022-11-10-QEUR22_FLUX06/imageJL1-66-13.jpg)

C部長 : “あの団体って、**「自称保守」**でしょ？結局、「保守」の存在のためにはコミュニズムが不可欠のような感じですね。”

QEU:FOUNDER ： “左があるから右がある。だから、2人は仲良し・・・。”


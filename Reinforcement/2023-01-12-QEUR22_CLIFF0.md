---
title: QEUR22_CLIFF0:ã€€Cliff_Walkingã§éŠã¼ã†ï¼ˆãã®ï¼‘ï¼‰
date: 2023-01-12
tags: ["QEUã‚·ã‚¹ãƒ†ãƒ ", "ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹", "Pythonè¨€èª", "Cliff Walking", "Pytorch", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "å¼·åŒ–å­¦ç¿’"]
excerpt: Juliaè¨€èªã‚’ä½¿ã£ãŸå¼·åŒ–å­¦ç¿’
---

## QEUR22_CLIFF0:ã€€Cliff_Walkingã§éŠã¼ã†ï¼ˆãã®ï¼‘ï¼‰

## ï½ã€€å†ã‚¹ã‚¿ãƒ¼ãƒˆã¸ï¼ï¼ã€€ï½

### ãƒ»ãƒ»ãƒ»ã€€å‰å›ã®ã¤ã¥ãã§ã™ã€€ãƒ»ãƒ»ãƒ»

QEU:FOUNDER(è¨­å®šå¹´é½¢65æ­³) ï¼š â€œã“ã‚Œã¯ã‚²ãƒ¼ãƒ ã®å ±é…¬ã®å±¥æ­´ã‚’è¦‹ã¦ã„ã‚‹ã®ã§ã‚ã£ã¦ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ã®æå¤±é‡æ¨ç§»ã‚’ã¿ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã§ã™ã€‚ãã‚Œã§ã‚‚ã€å°ç”Ÿã‚‚ã¡ã‚‡ã£ã¨ãƒ˜ãƒ³ã ã¨ã¯æ€ã„ã¾ã™ã€‚â€

![imageJRL1-9-1](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-1.jpg)

Då…ˆç”Ÿ(è¨­å®šå¹´é½¢65æ­³)  ï¼š â€œã§ã‚‚ã€æˆ‘ã€…ãŒå‰å›ã«Cliff_Walkingã§è¡Œã£ãŸã‚„ã‚Šæ–¹ã¯æ­£ã—ã„ã“ã¨ã¯åˆ†ã‹ã‚Šã¾ã—ãŸã­ã€‚ã‚‚ã¡ã‚ã‚“ã€ãƒã‚°ã®æœ‰ç„¡ã¯åˆ†ã‹ã‚‰ãªã„ã§ã™ãŒãƒ»ãƒ»ãƒ»ã€‚æ¬¡ã¯ã€ã©ã†ã—ã¾ã™ã‹ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œã¡ã‚‡ã£ã¨ã€è€ƒãˆä¸­ãƒ»ãƒ»ãƒ»ã€‚ã“ã®ç•ªçµ„ã¯ã€**ã€Œé«˜é½¢è€…ã«ã‚ˆã‚‹ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã€**ã§ã™ã€‚ã‚ˆã—ï¼æœ€åˆã‹ã‚‰ã‚„ã£ã¦ã¿ã‚‹ã‹ï¼ï¼â€

Då…ˆç”Ÿ ï¼š â€œãˆã£ï¼ï¼Ÿã‚ªãƒƒã‚µãƒ³ï¼ˆFOUNDERï¼‰ã®è¨€ã£ã¦ã„ã‚‹æ„å‘³ãŒåˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚â€

QEU:FOUNDER ï¼š â€œã„ããªã‚Šã€Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ™’ã™ã‚ˆï¼ãƒ‰ãƒ³ï¼ï¼å®Ÿè¡Œçµæœã‚‚è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚â€

```python
# ----------------
# CLIFF WALKING SIMULATIONã®å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# step1 : ç°¡å˜ãªå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™
# step1 : dqn_cw_pytorch(handmade_env).py
# ----------------
# æ•°å€¤è¨ˆç®—ã®ï¾—ï½²ï¾Œï¾ï¾—ï¾˜èª­ã¿è¾¼ã¿
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ----------------
import matplotlib.pyplot as plt
# %matplotlib inline
# =================================================
# difinition of Environment function(Cliff_Walking)
# =================================================
NX, NY = 4, 12
PStart = [3.0, 0.0]
PGoal  = [3.0, 11.0]
LRUD = [
	[0.0, -1.0],  # 0->left
	[0.0, 1.0],   # 1->right
	[-1.0, 0.0],  # 2->up
	[1.0, 0.0],   # 3->down
]

# ------
# Cliffã«è½ã¡ãŸã‹ã®åˆ¤å®š
def iscliff(p):
    x, y = p[0], p[1]
    if x == 3 and (y > 0 and y < NY-1):
        return True
    else:
        return False

# ------
# ã‚²ãƒ¼ãƒ ã‚¾ãƒ¼ãƒ³ã‹ã‚‰å¤–ã‚ŒãŸã‹ã®åˆ¤å®š
def isoutlayout(p):
    x, y = p[0], p[1]
    if x < 0 or x > 3 or y < 0 or y > 11:
        return True
    else:
        return False

# ------
# test_no1 -> It works!
p = [1.0, 1.0]
print("iscliff: ",iscliff(p))

# ------
# ENVIRONMENTï¼šãƒªã‚»ãƒƒãƒˆ
def cliff_reset():
    done, reward = False, 0    # initialize
    position = PStart
    Sx, Sy   = position[0], position[1]
    # ---------------------------
    # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹
    EuDist_S = round(np.sqrt((Sx - PStart[0]) ** 2 + (Sy - PStart[1]) ** 2),4)
    EuDist_G = round(np.sqrt((Sx - PGoal[0]) ** 2 + (Sy - PGoal[1]) ** 2),4)
    EuDist_T = EuDist_S + EuDist_G
    arr_dists = [EuDist_S, EuDist_G, EuDist_T]
    # ---------------------------
    # Stateã‚’è¨ˆç®—ã™ã‚‹
    state = np.array([Sx, Sy, EuDist_S, EuDist_G])
    
    return state, position, reward, done, arr_dists

# ------
# test_no2 -> It works!
state, position, reward, done, arr_dists = cliff_reset()
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))

# ------
# ENVIRONMENTï¼šã‚³ãƒã®å‹•ãã®å‡¦ç†
def cliff_env(position, action):
    done, reward = False, 0    # initialize
    Sx, Sy = position[0], position[1]
    Ax, Ay = LRUD[action][0], LRUD[action][1]
    # ---
    Sx_next = Sx + Ax
    Sy_next = Sy + Ay
    position_next = [Sx_next, Sy_next]
    # ---
    if position_next[0] == PGoal[0] and position_next[1] == PGoal[1]:
        done = True
        reward = 50
    elif iscliff(position_next) == True:
        position_next = PStart
        reward = -100
    elif isoutlayout(position_next) == True:
        position_next = position
        reward = -10      
    else:
        reward = -0.1
    # ---------------------------
    # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹
    Sx_next = position_next[0]
    Sy_next = position_next[1]
    EuDist_S = round(np.sqrt((Sx_next - PStart[0]) ** 2 + (Sy_next - PStart[1]) ** 2),4)
    EuDist_G = round(np.sqrt((Sx_next - PGoal[0]) ** 2 + (Sy_next - PGoal[1]) ** 2),4)
    EuDist_T = EuDist_S + EuDist_G
    arr_dists = [EuDist_S, EuDist_G, EuDist_T]
    # ---------------------------
    # Stateã‚’è¨ˆç®—ã™ã‚‹
    state_next = np.array([Sx_next, Sy_next, EuDist_S, EuDist_G])
    
    return state_next, position_next, reward, done, arr_dists
    
# ------
# test_no3 -> It works!
position = [1.0, 1.0]
state_next, position_next, reward, done, arr_dists = cliff_env(position, 0)
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state_next, position_next, reward, done, arr_dists))

# =================================================
# ENV-testç”¨
# =================================================
# ---
# [0.0, -1.0],  # 0->left
# [0.0, 1.0],   # 1->right
# [-1.0, 0.0],  # 2->up
# [1.0, 0.0],   # 3->down
# ---
state, position, reward, done, arr_dists = cliff_reset()
print("--- RESET ---")
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))

arr_action = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]
for i, action in enumerate(arr_action):
    state, position, reward, done, arr_dists = cliff_env(position, action)
    print("--- MOVE:{}-{} ---".format(i, action))
    print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))
 
# =================================================
# RANDOM.CHOICE - ã‚¢ãƒ³ãƒãƒ§ã‚³ç”¨
# =================================================
# Calc-Distance
def EuGoal_dist(position, action):

    Sx, Sy = position[0], position[1]
    Ax, Ay = LRUD[action][0], LRUD[action][1]
    # ---
    Sx_next = Sx + Ax
    Sy_next = Sy + Ay
    
    EuDist_G = (Sx_next - PGoal[0]) ** 2 + (Sy_next - PGoal[1]) ** 2

    return EuDist_G

# ------
# random_probability
def dist_prob(position):
    # ---
	# [0.0, -1.0],  # 0->left
	# [0.0, 1.0],   # 1->right
	# [-1.0, 0.0],  # 2->up
	# [1.0, 0.0],   # 3->down
    # ---
    if position == PStart:
        arr_prob = [0.1, 0.1, 0.7, 0.1]
    else:
        arr_dist, arr_prob = [], [0.2, 0.2, 0.2, 0.2]
        for action in [0,1,2,3]:
            EuDist_G = EuGoal_dist(position, action)
            arr_dist.append(EuDist_G)

        idx_min = np.argmin(arr_dist)
        arr_prob[idx_min] = 0.4

    return arr_prob 

# =================================================
# Deep Learning Model class
# =================================================
# PyTorchã®DLã®å®šç¾©
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        #self.fc2 = torch.nn.Linear(128, 128)
        #self.fc2.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, dim_output)
        self.fc2.weight.data.normal_(0, 0.1)

    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        #x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc2(x)
        return x

# =================================================
# Calculation class(2) : DQN_Solver:
# =================================================
# Solving the minesweeper in Deep Q-learning
class DQN_Solver:

    def __init__(self):

        # --------------------------------------------------
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ã‚¿
        self.memory     = deque(maxlen=MEMORY_CAPACITY)
        self.gamma      = GAMMA
        self.epsilon    = EPSILON
        self.e_decay    = EPDECAY
        self.e_min      = EPMIN
        self.learning_rate = LRATE
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()  # åˆ©ç”¨Netåˆ›å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œ: è¯„ä¼°ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.learn_step_counter = 0
        # --------------------------------------------------
        # Save and load the model via state_dict
        # self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    # def save_models(self):
    # torch.save(self.eval_net.state_dict(), file_output_model)

    # ----------------
    # ãƒ¡ãƒ¢ãƒªã‚’è“„ç©ã™ã‚‹
    def remember_memory(self, state, action, reward, state_next, done):
        self.memory.append((state, action, reward, state_next, done))

    # ----------------
    # å‘½ä»¤ã‚’é¸æŠã™ã‚‹
    def choose_action(self, state, iCnt_play):

        # ----------------------------------------
        # ç§»å‹•å‘½ä»¤
        # LRUD = [
        #    [0.0, -1.0],  # 0->left
        #    [0.0, 1.0],   # 1->right
        #    [-1.0, 0.0],  # 2->up
        #    [1.0, 0.0],   # 3->down
        # ----------------
        Qvalue = -0.0001
        # ----------------
        # æœ€é©å‘½ä»¤ã®é¸æŠ
        if self.epsilon <= random.random():
            # DQNã«ã‚ˆã‚‹é¸æŠ
            acType = "machine"
            for iCnt, action in enumerate([0,1,2,3]):
                # [array]-action ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
                if action == 0:
                    temp_s_a = np.hstack([state, [1, 0, 0, 0]])
                elif action == 1:
                    temp_s_a = np.hstack([state, [0, 1, 0, 0]])
                elif action == 2:
                    temp_s_a = np.hstack([state, [0, 0, 1, 0]])
                elif action == 3:
                    temp_s_a = np.hstack([state, [0, 0, 0, 1]])
                # ----
                if iCnt == 0:
                    mx_input = np.array([temp_s_a])  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                else:
                    mx_input = np.append(mx_input, np.array([temp_s_a]), axis=0)  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
            # print("----- mx_input -----")
            # print(mx_input)
            # --------------------------------------------------
            # generate new 'x'
            x_input_tensor = torch.from_numpy(mx_input).float()
            # predict 'y'
            with torch.no_grad():
                y_pred_tensor = self.eval_net(x_input_tensor)
            # convert tensor to numpy
            y_pred  = y_pred_tensor.data.numpy().flatten()
            Qvalue  = np.max(y_pred)
            a_order = np.argmax(y_pred)
        else:
            # ä¹±æ•°ã«ã‚ˆã‚‹é¸æŠ
            acType = "random"
            if iCnt_play < 1500:
                # ã‚¢ãƒ³ãƒãƒ§ã‚³ç”¨
                prob_choice = dist_prob([state[0], state[1]])
                a_order = np.random.choice([0,1,2,3], p=prob_choice)
            else:
                # é€šå¸¸ç”¨
                a_order = np.random.choice([0,1,2,3])

        return a_order, acType, Qvalue

    # ----------------
    # (REPLAY EXPERIENCE)å­¦ç¿’ã™ã‚‹
    def replay_experience(self, batch_size):

        # ç›®æ ‡ç½‘ç»œå‚æ•°æ›´æ–°
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:  # ä¸€å¼€å§‹è§¦å‘ï¼Œç„¶åæ¯100æ­¥è§¦å‘
            self.target_net.load_state_dict(self.eval_net.state_dict())  # å°†è¯„ä¼°ç½‘ç»œçš„å‚æ•°èµ‹ç»™ç›®æ ‡ç½‘ç»œ
        self.learn_step_counter += 1
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, state_next, done = minibatch[ibat]
            # [array]-action ã‚’çµåˆã—ã¾ã™ã€‚
            if action == 0:
                state_action_curr = np.hstack([state, [1, 0, 0, 0]])
            elif action == 1:
                state_action_curr = np.hstack([state, [0, 1, 0, 0]])
            elif action == 2:
                state_action_curr = np.hstack([state, [0, 0, 1, 0]])
            elif action == 3:
                state_action_curr = np.hstack([state, [0, 0, 0, 1]])
            # print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                mx_input = []  # çŠ¶æ…‹(state)ã¨è¡Œå‹•(action)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                for iCnt, imov in enumerate([0,1,2,3]):
                    # [array]-action ã‚’çµåˆã—ã¾ã™ã€‚
                    if imov == 0:
                        temp_s_a = np.hstack([state_next, [1, 0, 0, 0]])
                    elif imov == 1:
                        temp_s_a = np.hstack([state_next, [0, 1, 0, 0]])
                    elif imov == 2:
                        temp_s_a = np.hstack([state_next, [0, 0, 1, 0]])
                    elif imov == 3:
                        temp_s_a = np.hstack([state_next, [0, 0, 0, 1]])
                    # ----
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                    else:
                        mx_input = np.concatenate([mx_input, [temp_s_a]], axis=0)  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                # ----------------
                mx_input = np.array(mx_input)
                # print("--- mx_input ---")
                # print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
            else:
                X = np.append(X, np.array([state_action_curr]), axis=0)  # çŠ¶æ…‹(S)è¡Œå‹•(A)ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
            Y = np.append(Y, target_f)
        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        # print("state_action_next:",state_action_next)
        # print("q_target:",q_target)
        # --- building model ---
        q_eval = self.eval_net(state_action_next)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # Show progress
        # print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon, 5), round(loss.data.item(), 5)

# =================================================
# Calculation class(3) : Agent
# =================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ã®è¨­å®š
        self.epsilon    = EPSILON  # Îµ-Greedy
        # ---------------------------
        # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)
        self.arr_iplay      = []  # count game play    ãƒ—ãƒ¬ã‚¤ç•ªå·
        self.arr_maxturn    = []  # turn game play    ã‚¿ãƒ¼ãƒ³æ•°
        self.arr_maxscore   = []  # rl_score game play    å ±é…¬ã®ç·å’Œ
        self.arr_victory    = []  # victory    å‹åˆ©ã—ãŸã‹
        self.arr_loss       = []  # DQN-Experience Replayå­¦ç¿’
        self.arr_epsilon    = []  # Îµ-Greedy
        # ---------------------------
        # Qå€¤ã®åˆ†æç”¨(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)
        self.arr_numQV = []  # QVå€¤ã®Næ•°
        self.arr_maxQV = []  # QVå€¤ã®æœ€å¤§å€¤
        self.arr_q25QV = []  # QVå€¤ã®4åˆ†ã®1å€¤
        self.arr_q75QV = []  # QVå€¤ã®4åˆ†ã®3å€¤
        # ---------------------------
        # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®åˆ†æç”¨(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)
        self.arr_maxED = []  # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
        # print("starting simulation")
        # --------------------------------------------------
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
        for iCnt_play in range(num_episodes):  # num_episodes
            # ã‚²ãƒ¼ãƒ ã™ã‚‹
            maxturn, maxscore, flag_goal, maxEDist = self.get_episode(iCnt_play)
            # ----------
            # DQN-Experience Replayå­¦ç¿’
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # çµæœã®å‡ºåŠ›
            print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play,
                                                    maxturn, maxscore, flag_goal, val_epsilon, val_loss))
            # ----------
            # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)ã®è¿½åŠ 
            self.arr_iplay.append(iCnt_play)  # count game play    ãƒ—ãƒ¬ã‚¤ç•ªå·
            self.arr_maxturn.append(maxturn)  # max_turn   ã‚²ãƒ¼ãƒ ã®ã‚¿ãƒ¼ãƒ³æ•°
            self.arr_maxscore.append(maxscore)  # rl_score game play    æœ€çµ‚ãƒ—ãƒ¬ã‚¤ã‚¹ã‚³ã‚¢
            self.arr_victory.append(flag_goal)  # victory    å‹åˆ©ã—ãŸã‹
            self.arr_loss.append(val_loss)    # DQN-Experience Replayå­¦ç¿’
            self.arr_epsilon.append(val_epsilon)  # ã‚¤ãƒ—ã‚·ãƒ­ãƒ³
            # ----------
            # Qå€¤ã®ä¿ç®¡(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)
            self.arr_numQV.append(maxturn)  # QVå€¤ã®Næ•°
            self.arr_maxQV.append(np.max(self.arr_predQV))  # QVå€¤ã®æœ€å¤§å€¤
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))  # QVå€¤ã®4åˆ†ã®1å€¤
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))  # QVå€¤ã®4åˆ†ã®3å€¤
            # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®åˆ†æç”¨(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)
            self.arr_maxED.append(maxEDist)  # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
            # ----------
            # ã—ã°ã‚‰ãã™ã‚Œã°è¡¨ç¤ºãŒæ¶ˆãˆã¾ã™
            if iCnt_play % 50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # å­¦ç¿’å±¥æ­´ã‚’å‡ºåŠ›ã™ã‚‹
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã™ã‚‹
        self.show_graph()

```

**(å®Ÿè¡Œçµæœãã®ï¼‘)**

![imageJRL1-9-2](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-2.jpg)

```python

    # --------------------------------------------------
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é‹ç”¨ã™ã‚‹
    def get_episode(self, iCnt_play):

        # ---------------------------
        # æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ç¾¤
        Qvalue, reward, done, flag_goal = -0.0001, 0, False, "NA"
        maxturn, maxscore, maxEDist = 0, 0, 0
        # ---------------------------
        # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹)
        self.arr_iturn  = []  # ã‚¿ãƒ¼ãƒ³ãƒ»ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚¹ãƒˆ
        self.orders_row = []  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(row)
        self.orders_col = []  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(col)
        self.arr_orders = []  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(å‘½ä»¤)
        self.arr_acType = []  # æŒ‡ç¤ºã®ã‚¿ã‚¤ãƒ—
        self.arr_scores = []  # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
        self.arr_dones  = []  # ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ©ã‚°
        self.arr_EuDist = []  # ãƒ¦ãƒ¼ã‚°ãƒªãƒƒãƒ‰è·é›¢ã®ãƒªã‚¹ãƒˆ
        self.arr_predQV = []  # Qå€¤ã®ãƒªã‚¹ãƒˆ
        # ---------------------------
        # ã‚²ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹
        state, position, reward, done, arr_dists = cliff_reset()

        # ---------------------------
        # ã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹
        iCnt_turn = 0
        while True:
            # å‘½ä»¤(a_order)ã¨çŠ¶æ…‹(state)ã‚’ä½œæˆã™ã‚‹
            a_order, acType, Qvalue = dql_solver.choose_action(state, iCnt_play)
            # ---------------------------
            # ã‚²ãƒ¼ãƒ ã‚’ã‚‚ã†ä¸€æ­©é€²ã‚ã‚‹
            state_next, position_next, reward, done, arr_dists = cliff_env(position, a_order)
            # ã‚²ãƒ¼ãƒ ãŒå®Œäº†ã—ã¦ã„ã‚‹ã®ã‹
            if done   == True:
                flag_goal = "goal"
            #print("iCnt_play:{}, iCnt_turn:{}, position:{}, state:{}, flag_goal:{}, reward:{}, arr_dists:{}".format(iCnt_play, iCnt_turn, position, state, flag_goal, reward, arr_dists))

            # ---------------------------
            # è¨˜éŒ²ç”¨ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ã™ã‚‹
            self.arr_iturn.append(iCnt_turn)  # ã‚¿ãƒ¼ãƒ³ãƒ»ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚¹ãƒˆ
            self.orders_row.append(position[0])  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(ROW)
            self.orders_col.append(position[1])  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(COL)
            self.arr_orders.append(a_order)  # æŒ‡ç¤ºãƒªã‚¹ãƒˆ(å‘½ä»¤)
            self.arr_acType.append(acType)  # æŒ‡ç¤ºã®ã‚¿ã‚¤ãƒ—
            self.arr_scores.append(reward)  # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            self.arr_dones.append(done)  # ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ©ã‚°
            self.arr_EuDist.append(arr_dists[2])  # ãƒ¦ãƒ¼ã‚°ãƒªãƒƒãƒ‰è·é›¢ã®ãƒªã‚¹ãƒˆ
            self.arr_predQV.append(Qvalue)  # Qå€¤ã®ãƒªã‚¹ãƒˆ
            # ----------------
            # Experience Replayé…åˆ—ã«ä¿ç®¡ã™ã‚‹
            dql_solver.remember_memory(state, a_order, reward, state_next, done)
            # ----------------
            # å®Ÿè¡Œç¶™ç¶šã™ã‚‹ã‹åˆ†å²
            if done == True:
                # Game over
                break  # (game over rip)
            elif iCnt_play < 2000 and iCnt_turn > 30000:
                flag_goal = "too_long"
                # Game over
                break  # (game over rip)
            elif iCnt_play >= 2000 and iCnt_turn > 2000:
                flag_goal = "too_long"
                # Game over
                break  # (game over rip)
            else:
                # ----------------
                # count of game turn
                position    = position_next
                state       = state_next
                iCnt_turn   = iCnt_turn + 1

        # --------------------------------------------------
        # ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã®å¼•ç¶™ã
        maxturn, maxscore, maxEDist = iCnt_turn, np.sum(self.arr_scores), np.amax(self.arr_EuDist)

        return maxturn, round(maxscore,2), flag_goal, maxEDist

    # ----------------
    # å­¦ç¿’çµæœã®ã‚°ãƒ©ãƒ•åŒ–
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # ç§»å‹•å¹³å‡ã‚’ä»˜ã‘ã‚‹
        y_rolling = pd.Series(self.arr_maxED).rolling(window=12, center=True).mean()
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxEDist')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxEDist')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.arr_maxED, label="original", color="blue")
        ax2.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax2.legend(loc='best')
        # -----
        # ç§»å‹•å¹³å‡ã‚’ä»˜ã‘ã‚‹
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # ç§»å‹•å¹³å‡ã‚’ä»˜ã‘ã‚‹
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        # fig.savefig("./AAC_img.png")
        plt.show()
```


**(å®Ÿè¡Œçµæœãã®ï¼’)**

![imageJRL1-9-3](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-3.jpg)

```python

# =================================================
# main function
# =================================================
if __name__ == "__main__":

    # ---------------------------
    # ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®šã®åˆæœŸåŒ–
    dim_input  = 4 + 4  # action_onehot
    dim_output = 1
    # ---------------------------
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ã‚¿
    BATCH_SIZE = 128  # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
    LRATE = 0.001  # å­¦ç¿’ç‡
    EPSILON = 0.99  # greedy policy
    EPDECAY = 0.999
    EPMIN = 0.01
    GAMMA = 0.95  # reward discount
    TARGET_REPLACE_ITER = 100  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
    MEMORY_CAPACITY = 128 * 8  # ãƒ¡ãƒ¢ãƒªå®¹é‡
    # ---------------------------
    SLEEP_TIME = 0.01
    num_episodes = 2000  # ç¹°ã‚Šè¿”ã—å›æ•°

    # ---------------------------
    # ãƒ•ã‚©ãƒ«ãƒ€åã®æŒ‡å®š
    # foldername = "./ML_csvout/"  # My project folder

    # ---------------------------
    # dql_solverã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    dql_solver = DQN_Solver()

    # ---------------------------
    # å‡ºåŠ›ç”¨:Pytorchãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«å
    # comment_output_model = "initial"
    # code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # ãƒ¢ãƒ‡ãƒ«ã®æŒ‡å®š
    # file_output_model = foldername + code_output_model  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹åã®ç”Ÿæˆ

    # ---------------------------
    # DQN-ERã§å­¦ç¿’ã™ã‚‹
    Agent()

```

Då…ˆç”Ÿ ï¼š â€œã‚ã‚ãƒ»ãƒ»ãƒ»ã€æ‡ã‹ã—ã„ã§ã™ã­ãˆãƒ»ãƒ»ãƒ»ã€‚ç’°å¢ƒï¼ˆã‚²ãƒ¼ãƒ ï¼‰éƒ¨åˆ†ã«ã¤ã„ã¦ã¯è‡ªä½œã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸã‹ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œJuliaç‰ˆã®ç’°å¢ƒã‚’å°Šé‡ã—ãŸè¨­è¨ˆã§ã™ãƒ»ãƒ»ãƒ»(ç¬‘)ã€‚ã§ã‚‚ã­ã€Juliaç‰ˆã®ã‚²ãƒ¼ãƒ ã§ã¯å…¨ãå­¦ç¿’ãŒåæŸã—ãªã„ã§ã™ã‚ˆã€‚ã ã‹ã‚‰ã€ç›¸å½“å¤‰ãˆã¦ã„ã¾ã™ã€‚ã‚ã¨ã€Stateã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆé …ç›®ã®æƒ…å ±é‡ãŒä¸ååˆ†ãªã›ã„ã‹ã€åæŸã—ãŸã‚Šã—ãªã‹ã£ãŸã‚Šã¨ä¸å®‰å®šã§ã™ã€‚ãã®æ„å‘³ã§ã‚‚ã€ã„ã‚ã„ã‚ã€Œæ‰‹å“ã€ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚â€

Då…ˆç”Ÿ ï¼š â€œã‚ã–ã‚ã–Pythonã«ã¾ã§æˆ»ã£ã¦ã€ã“ã‚Œã‹ã‚‰ã©ã†ã™ã‚‹ã‚“ã§ã™ã‹ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œã“ã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã«ã€å¾ã€…ã«Juliaç‰ˆã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ãã¾ã™ã€‚ãã†ã™ã‚Œã°ã€æ¯”è¼ƒãŒæ¥½ã ã—ã€ReinforcementLearning.jlã®ã‚ã‚ŠãŒãŸã¿ãŒã‚ã‹ã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ãªããƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ ï¼š â€œã‚„ã£ã±ã‚Šã€Pythonç‰ˆã®ã‚°ãƒ©ãƒ•ç¾¤ã¯ãã‚Œã„ã ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER  ï¼š â€œã‚­ãƒ¬ã‚¤ãªã ã‘ã˜ã‚ƒãªã„ã‚ˆã€‚ä½•ãŒèµ·ã“ã£ã¦ã„ã‚‹ã®ã‹ãŒã‚ã‹ã‚Šã‚„ã™ã„ã‚“ã§ã™ã‚ˆã€‚ç‰¹ã«ã€QValueã®ã‚°ãƒ©ãƒ•ã‚’è¦‹ã‚Œã°ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã«å­¦ç¿’èƒ½åŠ›ãŒã‚ã‚‹ã®ã‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚â€

Då…ˆç”Ÿ ï¼š â€œãªã‚‹ã»ã©ï¼ï¼å­¦ç¿’æå¤±ã®å¤‰å‹•ã‚„ã€ã‚¹ã‚³ã‚¢ã®å¤‰å‹•ã‚’è¦‹ã¦ã‚‚ã‚ˆãã‚ã‹ã‚‰ãªã„ã§ã™ã‹ã‚‰ã­ã€‚â€

QEU:FOUNDER ï¼š â€œ**ã€ŒHookã€**ã£ã¦ã„ã†ã‚“ã ã£ã‘ã€å­¦ç¿’ã®çŠ¶æ…‹ã‚’æŠ½å‡ºã™ã‚‹æ©Ÿèƒ½ã£ã¦ãƒ»ãƒ»ãƒ»ã€‚Hookã«ã‚‚ã£ã¨ã„ã‚ã„ã‚ãªæ©Ÿèƒ½ãŒã‚ã‚Œã°ä¾¿åˆ©ãªã‚“ã§ã™ã‘ã©ã­ã€‚â€

Då…ˆç”Ÿ ï¼š â€œã§ãã‚Œã°ã€ãã“ã¾ã§ã‚„ã£ã¦ã¿ãŸã„ã§ã™ã­ã€‚ã§ãã‚Œã°ãƒ»ãƒ»ãƒ»ã€‚â€



## ï½ã€€ã¾ã¨ã‚ã€€ï½

## ãƒ»ãƒ»ãƒ»ã€€å‰å›ã®ã¤ã¥ãã§ã™ã€€ãƒ»ãƒ»ãƒ»

QEU:FOUNDER ï¼š â€œCå›½ã®B2Cãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯è£½é€ å´ã®å‰µé€ åŠ›ã‚’å¤§ããä¼¸ã°ã—ãŸåé¢ã€å›½å†…é‡è²©åº—ã€å°å£²åº—ãŒå…¨å›½çš„ã«å£Šæ»…ã—ã¾ã—ãŸã€‚çµå±€ã¯ã€Œè‰¯ã—ã‚ã—ã®å•é¡Œã€ãªã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚ãŸã ã—ã€å°ç”Ÿã¯Cå›½ã¯ã‚‚ã®ã¥ãã‚Šå¤§å›½ã§ã‚ã‚Šã¤ã¥ã‘ã‚‹ã¨æ€ã†ã‚ˆã€‚â€

[![MOVIE1](http://img.youtube.com/vi/aXzDXf-KJeY/0.jpg)](http://www.youtube.com/watch?v=aXzDXf-KJeY "æ·˜å¯¶é–‹ç®±ğŸ“¦ï½ 9ä»¶å¥½ç‰©æ²’å¾Œæ‚”è²·äº†å®ƒå€‘ï¼ğŸ˜ğŸ‘»28-1-2022")

Céƒ¨é•· : â€œCå›½ã¯ã€ã„ã¾ã ã«å‘¨ã‚Šã‹ã‚‰ãŸãŸã‹ã‚Œã¦ã„ã¾ã™ãŒãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL1-9-4](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-4.jpg)

QEU:FOUNDER ï¼š â€œCå›½ã¯ã€ãã®æŒã¡å‰ã®å‰µé€ åŠ›ã§ã€ãã®åœ§åŠ›ã‚’è·³ã­è¿”ã™ã‚“ã§ã™ã€‚æ˜”ã®Jå›½ã¿ãŸã„ã«ãƒ»ãƒ»ãƒ»ã€‚ä»Šã¾ã§ã®è­°è«–ã‚’ã‹ã‚“ãŒã¿ã¦ã€å‹•ç”»ã‚’è¦‹ã‚‹ã¨ãƒ»ãƒ»ãƒ»å¤±ç¬‘ãƒ»ãƒ»ãƒ»ãƒ»ã€‚â€

[![MOVIE2](http://img.youtube.com/vi/5fhNArCQAUA/0.jpg)](http://www.youtube.com/watch?v=5fhNArCQAUA "ã€çµŒæ¸ˆå¤§å±•æœ›2023ã€‘ä»Šå¹´ã¯è³ƒé‡‘ä¸Šæ˜‡ã®è»¢æ›ç‚¹ï¼GAFAã®ãƒˆãƒƒãƒ—äººæãŒæ—¥æœ¬ã§åƒãï¼æ—¥æœ¬ã¯G7ã§æœ€ã‚‚å®Ÿè³ªæˆé•·ç‡ãŒé«˜ã„ï¼ä½è³ƒé‡‘ãŒæŠ€è¡“é©æ–°ã‚’é…ã‚‰ã›ã‚‹ï¼è‹±èªã«é ¼ã‚‰ãªã„ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰å¯¾å¿œ")

QEU:FOUNDER ï¼š â€œã¡ãªã¿ã«ã€ã„ã¾ã¾ã§ã®è­°è«–ã«ã¯ã¡ã‚ƒã‚“ã¨èƒŒæ™¯(â†“)ãŒã‚ã‚‹ã‚“ã§ã™ã‚ˆã€‚ã“ã®æœ¬ã£ã¦ã€1993å¹´ã ã£ãŸã£ã‘ãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL1-9-5](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-5.jpg)

Céƒ¨é•· : â€œã„ã‚ã‚†ã‚‹ã€**ã€Œæœªæ¥å­¦è€…ã€**ã§ã—ãŸã£ã‘ã€‚å½¼ã¯ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œçŸ¥ã‚‰ã‚“ãƒ»ãƒ»ãƒ»ã€‚ã“ã®æœ¬ã€å°ç”Ÿã®ã†ã‚è¦šãˆã§ã¯ã€**çŸ¥è­˜ç¤¾ä¼šã«ãŠã‘ã‚‹ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã§ã¯ã€ä¸Šæµï¼ˆè£½é€ ï¼‰â€•ä¸‹æµï¼ˆå°å£²ï¼‰ã®ãƒ‘ãƒ¯ãƒ¼ã‚·ãƒ•ãƒˆãŒèµ·ã“ã‚‹**ã“ã¨ã‚’è¿°ã¹ã¦ã„ã¾ã™ã€‚ä¸‹æµï¼ˆå°å£²ï¼‰ã¯å¾“æ¥ã¯è£½é€ å´ã®ãƒ¢ãƒã‚’ãã®ã¾ã¾ä¸¦ã¹ã€ãŸã å£²ã‚‹ã“ã¨ã‚’è„±ã—ã€è‡ªåˆ†ãŒPOSãƒ‡ãƒ¼ã‚¿ãªã©ã§è“„ãˆãŸæƒ…å ±ã‚’æ­¦å™¨ã«ä¸Šæµã«ãƒ¢ãƒç”³ã™ã‚ˆã†ã«ãªã‚‹ã€‚æœ€å¾Œã¯ã€ã€Œè‡ªåˆ†ã§ãƒ¢ãƒã‚’ä½œã‚‹ã€ã‚ˆã†ã«ãªã‚‹ã¨ãƒ»ãƒ»ãƒ»ã€‚â€

Céƒ¨é•· : â€œDONKIã¯ã€Œæƒ…ç†±ä¾¡æ ¼ã€ã€AEONã¯ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œã§ã‚‚ã­ã€ç¾å®Ÿã¯æœªæ¥å­¦è€…ã‚’è¶…ãˆãŸã‚“ã§ã™ã‚ˆã­ã€‚**æ¶ˆè²»è€…ãŒç›´æ¥ã«ä¸Šæµã¨ãã£ã¤ã„ãŸï¼ï¼**â€

Céƒ¨é•· : â€œãã†ã™ã‚‹ã¨ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç”Ÿæ…‹ç³»ï¼‰ãŒå¤§ããå¤‰ã‚ã‚Šã¾ã—ãŸã€‚**ç”Ÿæ…‹ç³»ãŒå¤‰ã‚ã‚‹ã¨ã€ãã“ã«ä½ã‚€ç”Ÿç‰©ã‚‚å¤‰ã‚ã£ã¡ã‚ƒã†ã‚ˆã­**ã€‚Cå›½ã§ã¯é€²åŒ–ãŒç”Ÿã¾ã‚Œã€è£½é€ ãŒå¼·ããªã‚Šã¾ã—ãŸã€‚ä¸€æ–¹ã€Jå›½ã§ã¯é€²åŒ–ã¯é€²ã¾ãšã€ã„ã¾ã ã«ä¸‹æµãŒå¼·ã„çŠ¶æ³ãŒç¶­æŒã•ã‚Œã¦ã„ã¾ã™ã€‚â€

Céƒ¨é•· : â€œãã‚ŒãŒå®‰ã„Jå›½ã¨ã„ã†ã‚ã‘ã§ã™ã­ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œãŠå›½ãŒã‚‰ã¨ã„ã†ã‚ã‘ãƒ»ãƒ»ãƒ»ã€‚ãã‚ŒãŒã„ã„ã‹ã©ã†ã‹ã¯çŸ¥ã‚‰ã‚“ãƒ»ãƒ»ãƒ»ã€‚â€

